%
% Einführung in die Mustererkennung - WS2013
% Abgabeprotokoll Exercise 1
%
%

%{{{ misc
\documentclass[a4paper,psfig,subfigure,epsfig,fleqn,amssmb,float,caption,fontenc,ausarbeitung]{article}

\usepackage{graphicx}
\usepackage{epstopdf}
\usepackage{caption}
\usepackage{subcaption}

\usepackage{pgfplots}

%Zitieren:
\usepackage[english]{babel}
%\usepackage[german]{babel}
\usepackage{babelbib} % für das Erstellen des Bibtex-Literaturverzeichnisses
\usepackage{cite}
%\selectbiblanguage{english}
%\selectbiblanguage{german}


\usepackage{url}

\usepackage[]{mcode}

\usepackage[pdftitle={Einfuehrung in die Mustererkennung, Exercise 2},
            pdfauthor={David Pfahler},
						pdfauthor={Matthias Gusenbauer},
						pdfauthor={Matthias Vigele},
            pdfsubject={Mustererkennung},
            pdfborder={0 0 0}]{hyperref}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Titlepage

\pagestyle{empty}


%set dimensions of columns, gap between columns, and paragraph indent

\setlength{\textheight}{24.7 cm}
\setlength{\columnsep}{1 cm}
\setlength{\textwidth}{16 cm}
%\setlength{\footheight}{0.0 cm}
\setlength{\topmargin}{0.0 cm}
\setlength{\headheight}{0.0 cm}
\setlength{\headsep}{-0.3 cm}
\setlength{\oddsidemargin}{0.0 cm}
\setlength{\parindent}{0.7 cm}
\setlength{\mathindent}{0mm}

% set page counter if document is part of proceedings
\setcounter{page}{1}
\renewcommand{\floatpagefraction}{0.9}
\renewcommand{\textfraction}{0.1}

%\renewcommand{\captionlabelfont}{\fontfamily{phv}\fontseries{bx}\fontsize{10}{10pt}\selectfont}
%\renewcommand{\captionfont}{\fontfamily{phv}\fontsize{10}{12pt}\selectfont}
%\setlength{\captionmargin}{0.5 cm}

\makeatletter
\makeatother
\def\RR{\hbox{I\kern-.2em\hbox{R}}}


\begin{document}

%don't want date printed
\date{\today}

%make title bold and 14 pt font (Latex default is non-bold, 16pt) 
\title{~\\
  ~\\
  \fontsize{14}{14pt} \bf Abgabedokument Exercise 2
	 ~\\
  \fontsize{12}{12pt} \bf Einführung in die Mustererkennung 186.840 WS 2013}

%for single author 
\author{~\\
  ~\\
  \fontsize{12}{12pt}
  {\bf David Pfahler, Matthias Gusenbauer, Matthias Vigele}\\
  1126287, 1125577, 1126171
  ~\\ ~\\ ~\\
  \normalsize
}

\maketitle
%I don't know why I have to reset thispagestyle, but otherwise get page numbers 
\normalfont
\thispagestyle{empty}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% CONTENT

\section{Wine Classification - k-NN}
\label{sec:kNN}

This section presents the classification of different types of wines using 
the k-NN classification algorithm. The dataset\cite{data} contains the results of a 
chemical analysis of wines made in the same region in Italy but derived from 
three different cultivars. That means we have a three class problem. 
Section~\ref{sec:features} presents a method, how to find the best features 
for the classification. Section~\ref{sec:kNNtestset} shows how to separate 
the data into a training and a test set. Section~\ref{sec:kNNperformance} 
presents the performance of the classification and Section~\ref{sec:kNNResults}
evaluates it.


\subsection{Feature Extraction}
\label{sec:features}


	\begin{figure}
		\centering
			\includegraphics{img/boxplot.eps}
		\caption{Boxplots of all features of the dataset}
		\label{fig:boxes}
	\end{figure}

\begin{figure}
	\centering
	\newlength\figureheight 
	\newlength\figurewidth 
	\setlength\figureheight{7cm} 
	\setlength\figurewidth{9cm}
	\input{img/scatter.tikz}
	\caption{Scatterplot of the dataset with principal components}
	\label{fig:scatter}
\end{figure}

\begin{figure}
	\centering
	\setlength\figureheight{10cm} 
	\setlength\figurewidth{10cm}
	\input{img/inf.tikz}
	\caption{The main influences of the features to the first three principal components represented in an interactive 3D-graph}
	\label{fig:inf}
\end{figure}

In a chemical analysis of the wines, the following attributes were extracted:

\begin{enumerate}
	\item Alcohol 
	\item Malic acid 
	\item Ash 
	\item Alcalinity of ash 
	\item Magnesium 
	\item Total phenols 
	\item Flavanoids 
	\item Nonflavanoid phenols 
	\item Proanthocyanins 
	\item Color intensity 
	\item Hue 
	\item OD280/OD315 of diluted wines 
	\item Proline 
\end{enumerate}

This features vector could be represented by a 13-dimensional coordinate 
system. But due to the rule of thumb (\ref{eq:ruleOfThumb})presented in the 
lecture this are too many features for the size of the training set.
\begin{equation}
\label{eq:ruleOfThumb}
	\frac{n}{d} > 10
\end{equation}
This rule of thumb says that the size $n$ of the training set divided by the 
number of features $d$ should be greater than 10.

To reduce the number of features we analyzed the data. Figure~\ref{fig:boxes} 
shows a boxplot for each feature of the dataset. But first the dataset was 
standardized by dividing each feature by its standard deviation.

After that we decided to observe the dataset with the Principal Component 
Analysis (PCA) Figure~\ref{fig:scatter} show a scatterplot of the dataset 
with its two first principal components and the three classes are represented 
with different colors. This figure shows that it is possible to separate the 
three classes into three clusters. 

To find this clusters we looked at the main influences of the features to the 
first three principal components with an interactive \texttt{MATLAB} tool 
shown in Figure~\ref{fig:inf}. After some tests we found out that the best feature vector for the classification contains:

\begin{itemize}
	\item \textbf{Flavanoids}
	\item \textbf{Proline}
	\item \textbf{Color Intensity}
\end{itemize}


\subsection{Test and Training Set}
\label{sec:kNNtestset}

To determine a test and a training set we divided the smallest class (class 3)
 into two parts by a threshold. In our case the threshold is $0.5$, which 
means that the class with 48 samples gets divided into two 24 sample sets (a 
test set and a training set). All other classes are divided into a set with 
the same size as the previously calculated training set size and the rest 
gets into the test set.

To enhance the accuracy and reliability of the classification algorithm we 
haven't used just one training set, but we randomly shuffled the dataset to 
use 30 different training and test sets.


\subsection{Classification Performance}
\label{sec:kNNperformance}

TODO


\subsection{Evaluation of the Results}
\label{sec:kNNResults}

TODO


\section{Wine Classification - Mahalanobis Distance}
\label{sec:Mahalanobis}

TODO


\subsection{Test and Training Set}
\label{sec:MahalanobisTestSet}

TODO


\subsection{Classification Performance}
\label{sec:MahalanobisPerformance}

TODO


\subsection{Comparison with k-NN}
\label{sec:MahalanobisComparison}

TODO


\section{Discriminant Functions for the Normal Density}
\label{sec:DiscriminantFunctions}

TODO


\subsection{Computation of the Discriminate Function per Hand}
\label{sec:Hand}

TODO


\subsection{Computation of the Discriminate Function in \texttt{MATLAB}}
\label{sec:Matlab}

TODO


\pagebreak
% Biblography


\fontsize{9}{10pt}
\bibliographystyle{plain}
\bibliography{literatur}

\end{document}

% vim:foldmethod=marker
