%
% Einführung in die Mustererkennung - WS2013
% Abgabeprotokoll Exercise 1
%
%

%{{{ misc
\documentclass[subfigure,epsfig,fleqn,amssmb,float,caption,ausarbeitung]{scrartcl}

\usepackage{graphicx}
\usepackage{epstopdf}
\usepackage{caption}
\usepackage{subcaption}

\usepackage{pgfplots}

%Zitieren:
\usepackage[english]{babel}
%\usepackage[german]{babel}
\usepackage{babelbib} % für das Erstellen des Bibtex-Literaturverzeichnisses
\usepackage{cite}
%\selectbiblanguage{english}
%\selectbiblanguage{german}

\usepackage{url}

\usepackage[]{mcode}

\usepackage[pdftitle={Einfuehrung in die Mustererkennung, Exercise 2},
            pdfauthor={David Pfahler},
						pdfauthor={Matthias Gusenbauer},
						pdfauthor={Matthias Vigele},
            pdfsubject={Mustererkennung},
            pdfborder={0 0 0}]{hyperref}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Titlepage

\pagestyle{empty}


%set dimensions of columns, gap between columns, and paragraph indent

\setlength{\textheight}{24.7 cm}
\setlength{\columnsep}{1 cm}
\setlength{\textwidth}{16 cm}
%\setlength{\footheight}{0.0 cm}
\setlength{\topmargin}{0.0 cm}
\setlength{\headheight}{0.0 cm}
\setlength{\headsep}{-0.3 cm}
\setlength{\oddsidemargin}{0.0 cm}
\setlength{\parindent}{0 cm}
\setlength{\parskip}{0.5em}
\setlength{\mathindent}{0mm}

% set page counter if document is part of proceedings
\setcounter{page}{1}
\renewcommand{\floatpagefraction}{0.9}
\renewcommand{\textfraction}{0.1}

%\renewcommand{\captionlabelfont}{\fontfamily{phv}\fontseries{bx}\fontsize{10}{10pt}\selectfont}
%\renewcommand{\captionfont}{\fontfamily{phv}\fontsize{10}{12pt}\selectfont}
%\setlength{\captionmargin}{0.5 cm}

\makeatletter
\makeatother
\def\RR{\hbox{I\kern-.2em\hbox{R}}}


\begin{document}

%don't want date printed
\date{\today}

%make title bold and 14 pt font (Latex default is non-bold, 16pt) 
\title{~\\
  ~\\
  \fontsize{14}{14pt} \bf Abgabedokument Exercise 2
	 ~\\
  \fontsize{12}{12pt} \bf Einführung in die Mustererkennung 186.840 WS 2013}

%for single author 
\author{~\\
  ~\\
  \fontsize{12}{12pt}
  {\bf David Pfahler, Matthias Gusenbauer, Matthias Vigele}\\
  1126287, 1125577, 1126171
  ~\\ ~\\ ~\\
  \normalsize
}

\maketitle
%I don't know why I have to reset thispagestyle, but otherwise get page numbers 
\normalfont
\thispagestyle{empty}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% CONTENT

\section{Wine Classification - k-NN}
\label{sec:kNN}

This section presents the classification of different types of wines using 
the k-NN classification algorithm. The dataset\cite{data} contains the results of a 
chemical analysis of wines made in the same region in Italy but derived from 
three different cultivars. That means we have a three class problem. 
Section~\ref{sec:features} presents a method, how to find the best features 
for the classification. Section~\ref{sec:kNNtestset} shows how to separate 
the data into a training and a test set. Section~\ref{sec:kNNperformance} 
presents the performance of the classification and Section~\ref{sec:kNNResults}
evaluates it.


\subsection{Feature Extraction}
\label{sec:features}


	\begin{figure}
		\centering
			\includegraphics{img/boxplot.eps}
		\caption{Boxplots of all features of the dataset}
		\label{fig:boxes}
	\end{figure}

\begin{figure}
	\centering
	\newlength\figureheight 
	\newlength\figurewidth 
	\setlength\figureheight{7cm} 
	\setlength\figurewidth{9cm}
	\input{img/scatter.tikz}
	\caption{Scatterplot of the dataset with principal components}
	\label{fig:scatter}
\end{figure}

\begin{figure}
	\centering
	\setlength\figureheight{7cm} 
	\setlength\figurewidth{7cm}
	\input{img/inf.tikz}
	\caption{The main influences of the features to the first three principal components represented in an interactive 3D-graph}
	\label{fig:inf}
\end{figure}

In a chemical analysis of the wines, the following attributes were extracted:

\begin{enumerate}
	\item Alcohol 
	\item Malic acid 
	\item Ash 
	\item Alcalinity of ash 
	\item Magnesium 
	\item Total phenols 
	\item Flavanoids 
	\item Nonflavanoid phenols 
	\item Proanthocyanins 
	\item Color intensity 
	\item Hue 
	\item OD280/OD315 of diluted wines 
	\item Proline 
\end{enumerate}

This features vector could be represented by a 13-dimensional coordinate 
system. But due to the rule of thumb (\ref{eq:ruleOfThumb})presented in the 
lecture this are too many features for the size of the training set.
\begin{equation}
\label{eq:ruleOfThumb}
	\frac{n}{d} > 10
\end{equation}
This rule of thumb says that the size $n$ of the training set divided by the 
number of features $d$ should be greater than 10.

To reduce the number of features we analyzed the data. Figure~\ref{fig:boxes} 
shows a boxplot for each feature of the dataset. But first the dataset was 
standardized by dividing each feature by its standard deviation.

After that we decided to observe the dataset with the Principal Component 
Analysis (PCA) Figure~\ref{fig:scatter} show a scatterplot of the dataset 
with its two first principal components and the three classes are represented 
with different colors. This figure shows that it is possible to separate the 
three classes into three clusters. 

To find this clusters we looked at the main influences of the features to the 
first three principal components with an interactive \texttt{MATLAB} tool 
shown in Figure~\ref{fig:inf}. After some tests we found out that the best feature vector for the classification contains:

\begin{itemize}
	\item \textbf{Flavanoids}
	\item \textbf{Proline}
	\item \textbf{Color Intensity}
\end{itemize}


\subsection{Test and Training Set}
\label{sec:kNNtestset}

To determine a test and a training set we divided the smallest class (class 3)
 into two parts by a threshold. In our case the threshold is $0.5$, which 
means that the class with 48 samples gets divided into two 24 sample sets (a 
test set and a training set). All other classes are divided into a set with 
the same size as the previously calculated training set size and the rest 
gets into the test set.

To enhance the accuracy and reliability of the classification algorithm we 
haven't used just one training set, but we randomly shuffled the dataset to 
use 30 different combinations of training and test sets. With every training 
and test set we computed a new classification.


\subsection{Classification Performance}
\label{sec:kNNperformance}

\begin{figure}
	\centering
	\setlength\figureheight{6cm} 
	\setlength\figurewidth{7cm}
	\input{img/oneresult.tikz}
	\caption{The best result from the k-NN classification}
	\label{fig:bestresult}
\end{figure}

\begin{figure}
	\centering
	\setlength\figureheight{6cm} 
	\setlength\figurewidth{7cm}
	\input{img/allresults.tikz}
	\caption{All results from the k-NN classification with 30 different test and training sets}
	\label{fig:allresults}
\end{figure}

\begin{figure}
		\centering
			\includegraphics[width=9cm]{img/boxresults.eps}
		\caption{The upper Boxplot shows the derivation of the $k$ from the k-NN 
classifier. The Boxplot below shows the derivation of the classification performance}
		\label{fig:boxresults}
	\end{figure}

Figure~\ref{fig:bestresult} shows the best result of the k-NN classification 
with one of the test and training sets. But not all results are that good. 
Figure~\ref{fig:allresults} presents the performance of all 30 different test 
and training sets and their used $k$. Also the Median of the results is shown, 
which is $91,5094\%$ and the Median of the $k$ is $11$. But the median itself 
gives not a good representation of the derivation of the data. 
Figure~\ref{fig:boxresults} shows this information with boxplots.


\subsection{Evaluation of the Results}
\label{sec:kNNResults}

\begin{figure}
	\centering
	\setlength\figureheight{6cm} 
	\setlength\figurewidth{7cm}
	\input{img/ks.tikz}
	\caption{The median of the classification error rates of all 30 test sets for different values of $k$}
	\label{fig:ks}
\end{figure}

Our results show that the k-NN classifier is a powerful tool to classify the 
given data. But with a low value of $k$ the classification error enhances. Figure~\ref{fig:ks} shows the median of the classification error rates of all 30 test sets for different values of $k$. The smaller the error rate, the better the performance. Like we mentioned above the minimum is at a $k=11$.


\section{Wine Classification - Mahalanobis Distance}
\label{sec:Mahalanobis}

In the year 1936 Prasanta Chandra Mahalanobis introduced the Mahalanobis Distance which is a statistical measurement of distances between data points from a sample point. Mahalanobis Distance measures the similarity of samples with a known data set. Mathematically it is a measurement tool that is scale invariant and pays attention to data correlations.


\subsection{Test and Training Set}
\label{sec:MahalanobisTestSet}

The Test- and Training Sets for the classification with Mahalanobis Distance are equal to the sets used for the k-NN classification to create the same data basis. With an equal data basis it is easier to directly compare the results to each other. As state before the smallest class is divided by a threshold and then the same amount of data is taken from the other classes to get a training set. The remaining data samples not in the training sets are then accumulated into a single testset. This procedure is repeated 30 times to get a descriptive result of high quality. 


\subsection{Classification Performance}
\label{sec:MahalanobisPerformance}

It was found that with our seed for trainings set randomisation the performance of the Mahalanobis Distance Classifier got worse if a full covariance matrix was used during distance calculation. Interestingly the simpler the covariance matrix got the more accurate the results were. The code was double checked and even a test with the built in Matlab function($mahal()$) was conducted. The results remained the same and it is not possible to fully explain these results. The following will show the performance of the Mahalanobis Distance classifier. 
Another advantage of the Mahalanobis Distance classifier over kNN is the runtime performance. Compared to kNN the calculations done for the Mahalanobis classifier are fast which needs to be considered for the choice between kNN and the Mahalanobis Distance. \\

\begin{figure}[ht]
	\centering
	\includegraphics[scale=0.75]{img/mahalanobis_results_diag_identity.jpg}
	\caption{Mahalanobis performance with the diagonal identity matrix.}
\end{figure}

\begin{figure}
	\centering
	\includegraphics[scale=0.75]{img/mahalanobis_results_diag_covmat.jpg}
	\caption{Mahalanobis performance with the diagonal covariance matrix.}
\end{figure}

\begin{figure}
	\centering
	\includegraphics[scale=0.75]{img/mahalanobis_results_full_covmat.jpg}
	\caption{Mahalanobis performance with the full covariance matrix.}
\end{figure}

\begin{figure}
	\centering
	\includegraphics[scale=0.75]{img/mahalanobis_results_matlab_covmat.jpg}
	\caption{Mahalanobis performance with the built in Matlab function mahal() to validate the custom implementation.}
\end{figure}

\subsection{Comparison with k-NN}
\label{sec:MahalanobisComparison}

Compared to the kNN classifier the results show that the Mahalanobis classifier has better top performance with a success rate of 99.056\%. As seen in the figures of the success rate the Mahalanobis classifier is more sensible to the choice of trainings samples than kNN. On the other hand the performance drop of kNN at $2*trainingsSetSize$ is not given for the Mahalanobis classifier. 


\section{Discriminant Functions for the Normal Density}
\label{sec:DiscriminantFunctions}

TODO


\subsection{Computation of the Discriminate Function per Hand}
\label{sec:Hand}

TODO


\subsection{Computation of the Discriminate Function in \texttt{MATLAB}}
\label{sec:Matlab}

TODO


\pagebreak
% Biblography


\fontsize{9}{10pt}
\bibliographystyle{plain}
\bibliography{literatur}

\end{document}

% vim:foldmethod=marker
